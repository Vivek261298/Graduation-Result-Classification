{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383e0635-6cb0-4119-bab5-2386a7afa4b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0832f339-94cc-4771-8c96-e68bc029d264",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9834500-15d1-4576-a5cc-e2ed777c35d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9594b84-5f54-4e15-b3d6-8515482a7896",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ed80bc-fe0b-48ae-a579-ec011a36528e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=spark.read.csv(\"dbfs:/FileStore/tables/en_lpor_explorer.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce408444-ce64-4a0e-a4cd-9f1fb515032b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f8ed04f-63e7-4d11-9afd-bcb24d67dc7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=(spark.read.format(\"csv\")\n",
    "      .option(\"path\",\"dbfs:/FileStore/tables/en_lpor_explorer.csv\")\n",
    "      .option(\"header\",\"True\")\n",
    "      .option(\"inferSchema\",\"True\")\n",
    "      .load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1281f5bb-8b24-4963-b58f-4bc652d0658f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Descriptive Statistics Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82fca4ed-23cf-4dab-81dd-00059b689710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Basic MCT check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9059a2c3-47ad-406b-9f67-c51f56f61804",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>School</th><th>Gender</th><th>Age</th><th>Housing_Type</th><th>Family_Size</th><th>Parental_Status</th><th>Mother_Education</th><th>Father_Education</th><th>Mother_Work</th><th>Father_Work</th><th>Reason_School_Choice</th><th>Legal_Responsibility</th><th>Commute_Time</th><th>Weekly_Study_Time</th><th>Extra_Educational_Support</th><th>Parental_Educational_Support</th><th>Private_Tutoring</th><th>Extracurricular_Activities</th><th>Attended_Daycare</th><th>Desire_Graduate_Education</th><th>Has_Internet</th><th>Is_Dating</th><th>Good_Family_Relationship</th><th>Free_Time_After_School</th><th>Time_with_Friends</th><th>Alcohol_Weekdays</th><th>Alcohol_Weekends</th><th>Health_Status</th><th>School_Absence</th><th>Grade_1st_Semester</th><th>Grade_2nd_Semester</th></tr></thead><tbody><tr><td>count</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td><td>649</td></tr><tr><td>mean</td><td>null</td><td>null</td><td>16.7442218798151</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>3.6594761171032357</td><td>11.399075500770415</td><td>11.570107858243452</td></tr><tr><td>stddev</td><td>null</td><td>null</td><td>1.2181376394800634</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>4.640758808397153</td><td>2.7452651284463707</td><td>2.913638664303866</td></tr><tr><td>min</td><td>Gabriel Pereira</td><td>Female</td><td>15</td><td>Rural</td><td>Above 3</td><td>Living Together</td><td>High School</td><td>High School</td><td>Health</td><td>Health</td><td>Course Preference</td><td>Father</td><td>15 to 30 min</td><td>2 to 5h</td><td>No</td><td>No</td><td>No</td><td>No</td><td>No</td><td>No</td><td>No</td><td>No</td><td>Excellent</td><td>High</td><td>High</td><td>High</td><td>High</td><td>Fair</td><td>0</td><td>0</td><td>0</td></tr><tr><td>max</td><td>Mousinho da Silveira</td><td>Male</td><td>22</td><td>Urban</td><td>Up to 3</td><td>Separated</td><td>Primary School</td><td>Primary School</td><td>other</td><td>other</td><td>Reputation</td><td>Other</td><td>Up to 15 min</td><td>Up to 2h</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Very Poor</td><td>Very Low</td><td>Very Low</td><td>Very Low</td><td>Very Low</td><td>Very Poor</td><td>32</td><td>19</td><td>19</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649",
         "649"
        ],
        [
         "mean",
         null,
         null,
         "16.7442218798151",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "3.6594761171032357",
         "11.399075500770415",
         "11.570107858243452"
        ],
        [
         "stddev",
         null,
         null,
         "1.2181376394800634",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "4.640758808397153",
         "2.7452651284463707",
         "2.913638664303866"
        ],
        [
         "min",
         "Gabriel Pereira",
         "Female",
         "15",
         "Rural",
         "Above 3",
         "Living Together",
         "High School",
         "High School",
         "Health",
         "Health",
         "Course Preference",
         "Father",
         "15 to 30 min",
         "2 to 5h",
         "No",
         "No",
         "No",
         "No",
         "No",
         "No",
         "No",
         "No",
         "Excellent",
         "High",
         "High",
         "High",
         "High",
         "Fair",
         "0",
         "0",
         "0"
        ],
        [
         "max",
         "Mousinho da Silveira",
         "Male",
         "22",
         "Urban",
         "Up to 3",
         "Separated",
         "Primary School",
         "Primary School",
         "other",
         "other",
         "Reputation",
         "Other",
         "Up to 15 min",
         "Up to 2h",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Yes",
         "Very Poor",
         "Very Low",
         "Very Low",
         "Very Low",
         "Very Low",
         "Very Poor",
         "32",
         "19",
         "19"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "School",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Housing_Type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Family_Size",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Parental_Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mother_Education",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Father_Education",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mother_Work",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Father_Work",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Reason_School_Choice",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Legal_Responsibility",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Commute_Time",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Weekly_Study_Time",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Extra_Educational_Support",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Parental_Educational_Support",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Private_Tutoring",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Extracurricular_Activities",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Attended_Daycare",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Desire_Graduate_Education",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Has_Internet",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Is_Dating",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Good_Family_Relationship",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Free_Time_After_School",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Time_with_Friends",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Alcohol_Weekdays",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Alcohol_Weekends",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Health_Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "School_Absence",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Grade_1st_Semester",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Grade_2nd_Semester",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7296cd2e-84a7-4037-b34d-6ed4f517fbb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Null values in column check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a644c3cc-e526-4103-9d38-4efd111869ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No null values detected\n"
     ]
    }
   ],
   "source": [
    "def null_values(df):\n",
    "    for i in df.columns:\n",
    "        if df.filter(col(i).isNull()).count()>0:\n",
    "            print(i,df.filter(col(i).isNull()).count())\n",
    "    print('No null values detected')\n",
    "null_values(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4ead33-5301-410d-8a2b-3c2db246bb93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "#### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6f7d4b-c1ac-4f14-8b32-3a9eefc647cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc0173b-792f-4357-b7eb-b45c0abd8103",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f12a8b57-e753-4938-8a0c-d78626d59086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=data.withColumn('combined_score',col(\"Grade_1st_Semester\")+col(\"Grade_2nd_Semester\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ee5989c-e6c5-42b9-b168-cea59699c896",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=data.drop('Grade_1st_Semester','Grade_2nd_Semester')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723b0ff4-d108-467d-b669-e90da2eb2c68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=data.withColumn('result',when(col('combined_score')>=24,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c904d87-2ca7-4cb4-ae34-60130265cfeb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9854a0-86ee-4e9d-8755-5f79d0a6e0cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca400a1b-2c22-45f1-bef9-c986dc983a5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197ef28a-7d61-42a0-a15f-1701feb32673",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b971c2-508e-47f3-944e-526e3dde7035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    data=data.withColumn(f\"category_frequency_{i}\", (count(i).over(Window.partitionBy(i)))/data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42dfbe8e-0280-4f27-84a3-c41cafa42a10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba78a3fa-283e-46e4-8f17-ae2f3b65c66a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Selecting relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b006d5-3c02-4a7d-9b30-cb8b2ba34522",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2=data.select('category_frequency_School',\n",
    " 'category_frequency_Gender',\n",
    " 'category_frequency_Age',\n",
    " 'category_frequency_Housing_Type',\n",
    " 'category_frequency_Family_Size',\n",
    " 'category_frequency_Parental_Status',\n",
    " 'category_frequency_Mother_Education',\n",
    " 'category_frequency_Father_Education',\n",
    " 'category_frequency_Mother_Work',\n",
    " 'category_frequency_Father_Work',\n",
    " 'category_frequency_Reason_School_Choice',\n",
    " 'category_frequency_Legal_Responsibility',\n",
    " 'category_frequency_Commute_Time',\n",
    " 'category_frequency_Weekly_Study_Time',\n",
    " 'category_frequency_Extra_Educational_Support',\n",
    " 'category_frequency_Parental_Educational_Support',\n",
    " 'category_frequency_Private_Tutoring',\n",
    " 'category_frequency_Extracurricular_Activities',\n",
    " 'category_frequency_Attended_Daycare',\n",
    " 'category_frequency_Desire_Graduate_Education',\n",
    " 'category_frequency_Has_Internet',\n",
    " 'category_frequency_Is_Dating',\n",
    " 'category_frequency_Good_Family_Relationship',\n",
    " 'category_frequency_Free_Time_After_School',\n",
    " 'category_frequency_Time_with_Friends',\n",
    " 'category_frequency_Alcohol_Weekdays',\n",
    " 'category_frequency_Alcohol_Weekends',\n",
    " 'category_frequency_Health_Status',\n",
    " 'School_Absence','result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bffb553-3bec-41d7-9dfa-b46e98910a72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5009d1ec-473e-4d2e-8d53-55eb4cf62a6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0e249d-4250-4d42-8332-947fbbfe3b5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2=df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c4747d-2e0b-4ad8-b673-7b18cde9881b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=df2.drop('result',axis=1)\n",
    "y=df2['result']\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26717f8c-500b-4f14-a06f-9fb571237efa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,cohen_kappa_score,accuracy_score,recall_score,f1_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa46cae-1679-44d2-a591-4ac980d330b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: xgboost in /databricks/python3/lib/python3.10/site-packages (1.7.6)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from xgboost) (1.10.0)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from xgboost) (1.23.5)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2537d559-3321-4927-a048-ba7ded818a65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a452240-6c93-4212-9277-41b5de041912",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting mlflow\n  Downloading mlflow-2.9.2-py3-none-any.whl (19.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.1/19.1 MB 44.1 MB/s eta 0:00:00\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.4.39)\nRequirement already satisfied: packaging<24 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (22.0)\nRequirement already satisfied: gunicorn<22 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.10.0)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.5.3)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (8.0.4)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.7.0)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (4.11.3)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2.0.0)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (0.4)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (0.18.0)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2.2.5)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.1.2)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (0.4.2)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (6.0)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2022.7)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.1.27)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.1.1)\nCollecting alembic!=1.10.0,<2\n  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 230.6/230.6 kB 30.7 MB/s eta 0:00:00\nCollecting querystring-parser<2\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: pyarrow<15,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (8.0.0)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2.28.1)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.23.5)\nCollecting docker<7,>=4.0.0\n  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 13.7 MB/s eta 0:00:00\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.4.1)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (4.24.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.0)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.4.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.3.0)\nRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.10)\nRequirement already satisfied: urllib3<3,>=1.26.7 in /databricks/python3/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.14)\nRequirement already satisfied: websocket-client>=0.32.0 in /databricks/python3/lib/python3.10/site-packages (from docker<7,>=4.0.0->mlflow) (0.58.0)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow) (2.0.1)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.11)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.10/site-packages (from gunicorn<22->mlflow) (65.6.3)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.1)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.4.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.0.5)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow) (3.4)\nRequirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\nInstalling collected packages: querystring-parser, docker, alembic, mlflow\nSuccessfully installed alembic-1.13.0 docker-6.1.3 mlflow-2.9.2 querystring-parser-1.2.4\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b01541-216b-4404-9735-52970e976c93",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n[CV 1/3] END C=0.001, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=l2, solver=liblinear;, score=0.486 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l2, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l2, solver=liblinear;, score=0.225 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.001, max_iter=50, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=0.001, max_iter=50, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=0.001, max_iter=50, penalty=none, solver=newton-cg;, score=0.642 total time=   0.2s\n[CV 1/3] END C=0.001, max_iter=50, penalty=none, solver=lbfgs;, score=0.588 total time=   0.1s\n[CV 2/3] END C=0.001, max_iter=50, penalty=none, solver=lbfgs;, score=0.634 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.001, max_iter=50, penalty=none, solver=lbfgs;, score=0.578 total time=   0.1s\n[CV 1/3] END C=0.001, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=none, solver=sag;, score=0.601 total time=   0.1s\n[CV 2/3] END C=0.001, max_iter=50, penalty=none, solver=sag;, score=0.643 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=none, solver=sag;, score=0.472 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=50, penalty=none, solver=saga;, score=0.601 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=50, penalty=none, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=50, penalty=none, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.001, max_iter=100, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l2, solver=liblinear;, score=0.486 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l2, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=l2, solver=liblinear;, score=0.225 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=0.001, max_iter=100, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.001, max_iter=100, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n[CV 1/3] END C=0.001, max_iter=100, penalty=none, solver=lbfgs;, score=0.599 total time=   0.1s\n[CV 2/3] END C=0.001, max_iter=100, penalty=none, solver=lbfgs;, score=0.625 total time=   0.1s\n[CV 3/3] END C=0.001, max_iter=100, penalty=none, solver=lbfgs;, score=0.638 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.001, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=none, solver=sag;, score=0.606 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=none, solver=sag;, score=0.667 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=none, solver=sag;, score=0.524 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=100, penalty=none, solver=saga;, score=0.597 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=100, penalty=none, solver=saga;, score=0.647 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=100, penalty=none, solver=saga;, score=0.472 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=0.001, max_iter=200, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.1s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l2, solver=liblinear;, score=0.486 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=l2, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l2, solver=liblinear;, score=0.225 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.001, max_iter=200, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=0.001, max_iter=200, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=0.001, max_iter=200, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.001, max_iter=200, penalty=none, solver=lbfgs;, score=0.584 total time=   0.1s\n[CV 2/3] END C=0.001, max_iter=200, penalty=none, solver=lbfgs;, score=0.625 total time=   0.2s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.001, max_iter=200, penalty=none, solver=lbfgs;, score=0.632 total time=   0.2s\n[CV 1/3] END C=0.001, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.001, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=none, solver=sag;, score=0.600 total time=   0.1s\n[CV 2/3] END C=0.001, max_iter=200, penalty=none, solver=sag;, score=0.653 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=none, solver=sag;, score=0.562 total time=   0.0s\n[CV 1/3] END C=0.001, max_iter=200, penalty=none, solver=saga;, score=0.606 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=0.001, max_iter=200, penalty=none, solver=saga;, score=0.667 total time=   0.0s\n[CV 3/3] END C=0.001, max_iter=200, penalty=none, solver=saga;, score=0.540 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=l2, solver=newton-cg;, score=0.507 total time=   0.1s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.01, max_iter=50, penalty=l2, solver=newton-cg;, score=0.441 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=l2, solver=lbfgs;, score=0.507 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=l2, solver=lbfgs;, score=0.441 total time=   0.1s\n[CV 1/3] END C=0.01, max_iter=50, penalty=l2, solver=liblinear;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l2, solver=liblinear;, score=0.173 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=l2, solver=liblinear;, score=0.435 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.01, max_iter=50, penalty=l2, solver=sag;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=l2, solver=sag;, score=0.433 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=l2, solver=saga;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=l2, solver=saga;, score=0.028 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=l2, solver=saga;, score=0.426 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=none, solver=newton-cg;, score=0.584 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=0.01, max_iter=50, penalty=none, solver=newton-cg;, score=0.642 total time=   0.2s\n[CV 1/3] END C=0.01, max_iter=50, penalty=none, solver=lbfgs;, score=0.588 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=0.01, max_iter=50, penalty=none, solver=lbfgs;, score=0.634 total time=   0.1s\n[CV 3/3] END C=0.01, max_iter=50, penalty=none, solver=lbfgs;, score=0.578 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=none, solver=sag;, score=0.597 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=none, solver=sag;, score=0.633 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=none, solver=sag;, score=0.472 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=50, penalty=none, solver=saga;, score=0.601 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=50, penalty=none, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=50, penalty=none, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l2, solver=newton-cg;, score=0.507 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l2, solver=newton-cg;, score=0.441 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l2, solver=lbfgs;, score=0.507 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l2, solver=lbfgs;, score=0.441 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l2, solver=liblinear;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l2, solver=liblinear;, score=0.173 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l2, solver=liblinear;, score=0.435 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l2, solver=sag;, score=0.507 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l2, solver=sag;, score=0.437 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=l2, solver=saga;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=l2, solver=saga;, score=0.433 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=0.01, max_iter=100, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.01, max_iter=100, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n[CV 1/3] END C=0.01, max_iter=100, penalty=none, solver=lbfgs;, score=0.599 total time=   0.1s\n[CV 2/3] END C=0.01, max_iter=100, penalty=none, solver=lbfgs;, score=0.625 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.01, max_iter=100, penalty=none, solver=lbfgs;, score=0.638 total time=   0.1s\n[CV 1/3] END C=0.01, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=none, solver=sag;, score=0.606 total time=   0.1s\n[CV 2/3] END C=0.01, max_iter=100, penalty=none, solver=sag;, score=0.667 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=100, penalty=none, solver=sag;, score=0.535 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=100, penalty=none, solver=saga;, score=0.597 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=100, penalty=none, solver=saga;, score=0.638 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.01, max_iter=100, penalty=none, solver=saga;, score=0.472 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l2, solver=newton-cg;, score=0.507 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l2, solver=newton-cg;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=l2, solver=newton-cg;, score=0.441 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l2, solver=lbfgs;, score=0.507 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l2, solver=lbfgs;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=l2, solver=lbfgs;, score=0.441 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l2, solver=liblinear;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l2, solver=liblinear;, score=0.173 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=l2, solver=liblinear;, score=0.435 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l2, solver=sag;, score=0.507 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l2, solver=sag;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=l2, solver=sag;, score=0.437 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=l2, solver=saga;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=l2, solver=saga;, score=0.000 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.01, max_iter=200, penalty=l2, solver=saga;, score=0.433 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=0.01, max_iter=200, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.01, max_iter=200, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n[CV 1/3] END C=0.01, max_iter=200, penalty=none, solver=lbfgs;, score=0.584 total time=   0.2s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=0.01, max_iter=200, penalty=none, solver=lbfgs;, score=0.625 total time=   0.2s\n[CV 3/3] END C=0.01, max_iter=200, penalty=none, solver=lbfgs;, score=0.632 total time=   0.2s\n[CV 1/3] END C=0.01, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.01, max_iter=200, penalty=none, solver=sag;, score=0.600 total time=   0.1s\n[CV 2/3] END C=0.01, max_iter=200, penalty=none, solver=sag;, score=0.657 total time=   0.1s\n[CV 3/3] END C=0.01, max_iter=200, penalty=none, solver=sag;, score=0.574 total time=   0.0s\n[CV 1/3] END C=0.01, max_iter=200, penalty=none, solver=saga;, score=0.606 total time=   0.0s\n[CV 2/3] END C=0.01, max_iter=200, penalty=none, solver=saga;, score=0.667 total time=   0.0s\n[CV 3/3] END C=0.01, max_iter=200, penalty=none, solver=saga;, score=0.547 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=0.1, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l1, solver=liblinear;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=l1, solver=liblinear;, score=0.419 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l1, solver=saga;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=l1, solver=saga;, score=0.419 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l2, solver=newton-cg;, score=0.588 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=l2, solver=newton-cg;, score=0.549 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=l2, solver=newton-cg;, score=0.404 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l2, solver=lbfgs;, score=0.588 total time=   0.1s\n[CV 2/3] END C=0.1, max_iter=50, penalty=l2, solver=lbfgs;, score=0.549 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=l2, solver=lbfgs;, score=0.404 total time=   0.1s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l2, solver=liblinear;, score=0.626 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=l2, solver=liblinear;, score=0.517 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=l2, solver=liblinear;, score=0.446 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l2, solver=sag;, score=0.621 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=l2, solver=sag;, score=0.508 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.1, max_iter=50, penalty=l2, solver=sag;, score=0.441 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=l2, solver=saga;, score=0.600 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=l2, solver=saga;, score=0.487 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=l2, solver=saga;, score=0.443 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=0.1, max_iter=50, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=0.1, max_iter=50, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n[CV 1/3] END C=0.1, max_iter=50, penalty=none, solver=lbfgs;, score=0.588 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=0.1, max_iter=50, penalty=none, solver=lbfgs;, score=0.634 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=none, solver=lbfgs;, score=0.578 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=none, solver=sag;, score=0.597 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=none, solver=sag;, score=0.638 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=none, solver=sag;, score=0.472 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=50, penalty=none, solver=saga;, score=0.601 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=50, penalty=none, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=50, penalty=none, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.1, max_iter=100, penalty=l1, solver=liblinear;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=l1, solver=liblinear;, score=0.419 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=l1, solver=saga;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=l1, solver=saga;, score=0.419 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=l2, solver=newton-cg;, score=0.588 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l2, solver=newton-cg;, score=0.549 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=l2, solver=newton-cg;, score=0.404 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=l2, solver=lbfgs;, score=0.588 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l2, solver=lbfgs;, score=0.549 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=l2, solver=lbfgs;, score=0.404 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=l2, solver=liblinear;, score=0.626 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l2, solver=liblinear;, score=0.517 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=l2, solver=liblinear;, score=0.446 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=l2, solver=sag;, score=0.611 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l2, solver=sag;, score=0.533 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.1, max_iter=100, penalty=l2, solver=sag;, score=0.397 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=l2, solver=saga;, score=0.621 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=l2, solver=saga;, score=0.521 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=l2, solver=saga;, score=0.441 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=0.1, max_iter=100, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.1, max_iter=100, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n[CV 1/3] END C=0.1, max_iter=100, penalty=none, solver=lbfgs;, score=0.599 total time=   0.1s\n[CV 2/3] END C=0.1, max_iter=100, penalty=none, solver=lbfgs;, score=0.625 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.1, max_iter=100, penalty=none, solver=lbfgs;, score=0.638 total time=   0.1s\n[CV 1/3] END C=0.1, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=none, solver=sag;, score=0.606 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=none, solver=sag;, score=0.667 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=none, solver=sag;, score=0.558 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=100, penalty=none, solver=saga;, score=0.597 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=100, penalty=none, solver=saga;, score=0.638 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=100, penalty=none, solver=saga;, score=0.472 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.1, max_iter=200, penalty=l1, solver=liblinear;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l1, solver=liblinear;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=l1, solver=liblinear;, score=0.419 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=l1, solver=saga;, score=0.503 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l1, solver=saga;, score=0.000 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=l1, solver=saga;, score=0.419 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=l2, solver=newton-cg;, score=0.588 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l2, solver=newton-cg;, score=0.549 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.1, max_iter=200, penalty=l2, solver=newton-cg;, score=0.404 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=l2, solver=lbfgs;, score=0.588 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l2, solver=lbfgs;, score=0.549 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=l2, solver=lbfgs;, score=0.404 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=l2, solver=liblinear;, score=0.626 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l2, solver=liblinear;, score=0.517 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=l2, solver=liblinear;, score=0.446 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.1, max_iter=200, penalty=l2, solver=sag;, score=0.590 total time=   0.1s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l2, solver=sag;, score=0.539 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=l2, solver=sag;, score=0.397 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=l2, solver=saga;, score=0.611 total time=   0.1s\n[CV 2/3] END C=0.1, max_iter=200, penalty=l2, solver=saga;, score=0.533 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.1, max_iter=200, penalty=l2, solver=saga;, score=0.397 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=none, solver=newton-cg;, score=0.584 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=0.1, max_iter=200, penalty=none, solver=newton-cg;, score=0.642 total time=   0.2s\n[CV 1/3] END C=0.1, max_iter=200, penalty=none, solver=lbfgs;, score=0.584 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=0.1, max_iter=200, penalty=none, solver=lbfgs;, score=0.625 total time=   0.1s\n[CV 3/3] END C=0.1, max_iter=200, penalty=none, solver=lbfgs;, score=0.632 total time=   0.1s\n[CV 1/3] END C=0.1, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=0.1, max_iter=200, penalty=none, solver=sag;, score=0.600 total time=   0.1s\n[CV 2/3] END C=0.1, max_iter=200, penalty=none, solver=sag;, score=0.662 total time=   0.0s\n[CV 3/3] END C=0.1, max_iter=200, penalty=none, solver=sag;, score=0.585 total time=   0.0s\n[CV 1/3] END C=0.1, max_iter=200, penalty=none, solver=saga;, score=0.606 total time=   0.0s\n[CV 2/3] END C=0.1, max_iter=200, penalty=none, solver=saga;, score=0.667 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=0.1, max_iter=200, penalty=none, solver=saga;, score=0.540 total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l1, solver=liblinear;, score=0.603 total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=l1, solver=liblinear;, score=0.671 total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l1, solver=liblinear;, score=0.543 total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l1, solver=saga;, score=0.607 total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=l1, solver=saga;, score=0.517 total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l1, solver=saga;, score=0.463 total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l2, solver=newton-cg;, score=0.609 total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=l2, solver=newton-cg;, score=0.648 total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l2, solver=newton-cg;, score=0.531 total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l2, solver=lbfgs;, score=0.609 total time=   0.1s\n[CV 2/3] END C=1, max_iter=50, penalty=l2, solver=lbfgs;, score=0.657 total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l2, solver=lbfgs;, score=0.531 total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l2, solver=liblinear;, score=0.604 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, max_iter=50, penalty=l2, solver=liblinear;, score=0.667 total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l2, solver=liblinear;, score=0.540 total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l2, solver=sag;, score=0.587 total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=l2, solver=sag;, score=0.617 total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l2, solver=sag;, score=0.468 total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=l2, solver=saga;, score=0.611 total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=l2, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=l2, solver=saga;, score=0.450 total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, max_iter=50, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=1, max_iter=50, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=1, max_iter=50, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, max_iter=50, penalty=none, solver=lbfgs;, score=0.588 total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=none, solver=lbfgs;, score=0.634 total time=   0.1s\n[CV 3/3] END C=1, max_iter=50, penalty=none, solver=lbfgs;, score=0.578 total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=50, penalty=none, solver=sag;, score=0.597 total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=none, solver=sag;, score=0.638 total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=none, solver=sag;, score=0.472 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, max_iter=50, penalty=none, solver=saga;, score=0.601 total time=   0.0s\n[CV 2/3] END C=1, max_iter=50, penalty=none, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=1, max_iter=50, penalty=none, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=l1, solver=liblinear;, score=0.603 total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l1, solver=liblinear;, score=0.671 total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=l1, solver=liblinear;, score=0.543 total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=l1, solver=saga;, score=0.599 total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l1, solver=saga;, score=0.626 total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=l1, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=l2, solver=newton-cg;, score=0.609 total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l2, solver=newton-cg;, score=0.648 total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=l2, solver=newton-cg;, score=0.531 total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=l2, solver=lbfgs;, score=0.609 total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l2, solver=lbfgs;, score=0.648 total time=   0.1s\n[CV 3/3] END C=1, max_iter=100, penalty=l2, solver=lbfgs;, score=0.531 total time=   0.1s\n[CV 1/3] END C=1, max_iter=100, penalty=l2, solver=liblinear;, score=0.604 total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l2, solver=liblinear;, score=0.667 total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=l2, solver=liblinear;, score=0.540 total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=l2, solver=sag;, score=0.610 total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l2, solver=sag;, score=0.643 total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=l2, solver=sag;, score=0.500 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, max_iter=100, penalty=l2, solver=saga;, score=0.597 total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=l2, solver=saga;, score=0.621 total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=l2, solver=saga;, score=0.468 total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=1, max_iter=100, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=1, max_iter=100, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n[CV 1/3] END C=1, max_iter=100, penalty=none, solver=lbfgs;, score=0.599 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, max_iter=100, penalty=none, solver=lbfgs;, score=0.625 total time=   0.1s\n[CV 3/3] END C=1, max_iter=100, penalty=none, solver=lbfgs;, score=0.638 total time=   0.1s\n[CV 1/3] END C=1, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=none, solver=sag;, score=0.606 total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=none, solver=sag;, score=0.667 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, max_iter=100, penalty=none, solver=sag;, score=0.547 total time=   0.0s\n[CV 1/3] END C=1, max_iter=100, penalty=none, solver=saga;, score=0.597 total time=   0.0s\n[CV 2/3] END C=1, max_iter=100, penalty=none, solver=saga;, score=0.638 total time=   0.0s\n[CV 3/3] END C=1, max_iter=100, penalty=none, solver=saga;, score=0.472 total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=l1, solver=liblinear;, score=0.603 total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=l1, solver=liblinear;, score=0.671 total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=l1, solver=liblinear;, score=0.543 total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=l1, solver=saga;, score=0.621 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, max_iter=200, penalty=l1, solver=saga;, score=0.633 total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=l1, solver=saga;, score=0.439 total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=l2, solver=newton-cg;, score=0.609 total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=l2, solver=newton-cg;, score=0.648 total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=l2, solver=newton-cg;, score=0.531 total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=l2, solver=lbfgs;, score=0.609 total time=   0.2s\n[CV 2/3] END C=1, max_iter=200, penalty=l2, solver=lbfgs;, score=0.648 total time=   0.1s\n[CV 3/3] END C=1, max_iter=200, penalty=l2, solver=lbfgs;, score=0.531 total time=   0.1s\n[CV 1/3] END C=1, max_iter=200, penalty=l2, solver=liblinear;, score=0.604 total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=l2, solver=liblinear;, score=0.667 total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=l2, solver=liblinear;, score=0.540 total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=l2, solver=sag;, score=0.600 total time=   0.1s\n[CV 2/3] END C=1, max_iter=200, penalty=l2, solver=sag;, score=0.662 total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=l2, solver=sag;, score=0.562 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, max_iter=200, penalty=l2, solver=saga;, score=0.610 total time=   0.1s\n[CV 2/3] END C=1, max_iter=200, penalty=l2, solver=saga;, score=0.633 total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=l2, solver=saga;, score=0.500 total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=1, max_iter=200, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=1, max_iter=200, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=1, max_iter=200, penalty=none, solver=lbfgs;, score=0.584 total time=   0.3s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, max_iter=200, penalty=none, solver=lbfgs;, score=0.625 total time=   0.2s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=1, max_iter=200, penalty=none, solver=lbfgs;, score=0.632 total time=   0.3s\n[CV 1/3] END C=1, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=1, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=none, solver=sag;, score=0.600 total time=   0.1s\n[CV 2/3] END C=1, max_iter=200, penalty=none, solver=sag;, score=0.653 total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=none, solver=sag;, score=0.562 total time=   0.0s\n[CV 1/3] END C=1, max_iter=200, penalty=none, solver=saga;, score=0.606 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=1, max_iter=200, penalty=none, solver=saga;, score=0.667 total time=   0.0s\n[CV 3/3] END C=1, max_iter=200, penalty=none, solver=saga;, score=0.535 total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=l1, solver=liblinear;, score=0.593 total time=   0.2s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10, max_iter=50, penalty=l1, solver=liblinear;, score=0.644 total time=   0.1s\n[CV 3/3] END C=10, max_iter=50, penalty=l1, solver=liblinear;, score=0.607 total time=   0.1s\n[CV 1/3] END C=10, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=l1, solver=saga;, score=0.601 total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=l1, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=l1, solver=saga;, score=0.446 total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=l2, solver=newton-cg;, score=0.619 total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=l2, solver=newton-cg;, score=0.635 total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=l2, solver=newton-cg;, score=0.578 total time=   0.1s\n[CV 1/3] END C=10, max_iter=50, penalty=l2, solver=lbfgs;, score=0.609 total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=l2, solver=lbfgs;, score=0.639 total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=l2, solver=lbfgs;, score=0.586 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10, max_iter=50, penalty=l2, solver=liblinear;, score=0.588 total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=l2, solver=liblinear;, score=0.635 total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=l2, solver=liblinear;, score=0.591 total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=l2, solver=sag;, score=0.597 total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=l2, solver=sag;, score=0.633 total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=l2, solver=sag;, score=0.472 total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=l2, solver=saga;, score=0.611 total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=l2, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=l2, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10, max_iter=50, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=10, max_iter=50, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=10, max_iter=50, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10, max_iter=50, penalty=none, solver=lbfgs;, score=0.588 total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=none, solver=lbfgs;, score=0.634 total time=   0.1s\n[CV 3/3] END C=10, max_iter=50, penalty=none, solver=lbfgs;, score=0.578 total time=   0.1s\n[CV 1/3] END C=10, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=50, penalty=none, solver=sag;, score=0.597 total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=none, solver=sag;, score=0.643 total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=none, solver=sag;, score=0.472 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10, max_iter=50, penalty=none, solver=saga;, score=0.611 total time=   0.0s\n[CV 2/3] END C=10, max_iter=50, penalty=none, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=10, max_iter=50, penalty=none, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=l1, solver=liblinear;, score=0.593 total time=   0.2s\n[CV 2/3] END C=10, max_iter=100, penalty=l1, solver=liblinear;, score=0.644 total time=   0.2s\n[CV 3/3] END C=10, max_iter=100, penalty=l1, solver=liblinear;, score=0.607 total time=   0.1s\n[CV 1/3] END C=10, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=l1, solver=saga;, score=0.597 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10, max_iter=100, penalty=l1, solver=saga;, score=0.627 total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=l1, solver=saga;, score=0.472 total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=l2, solver=newton-cg;, score=0.619 total time=   0.1s\n[CV 2/3] END C=10, max_iter=100, penalty=l2, solver=newton-cg;, score=0.635 total time=   0.1s\n[CV 3/3] END C=10, max_iter=100, penalty=l2, solver=newton-cg;, score=0.578 total time=   0.1s\n[CV 1/3] END C=10, max_iter=100, penalty=l2, solver=lbfgs;, score=0.614 total time=   0.1s\n[CV 2/3] END C=10, max_iter=100, penalty=l2, solver=lbfgs;, score=0.639 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10, max_iter=100, penalty=l2, solver=lbfgs;, score=0.578 total time=   0.1s\n[CV 1/3] END C=10, max_iter=100, penalty=l2, solver=liblinear;, score=0.588 total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=l2, solver=liblinear;, score=0.635 total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=l2, solver=liblinear;, score=0.591 total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=l2, solver=sag;, score=0.606 total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=l2, solver=sag;, score=0.667 total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=l2, solver=sag;, score=0.524 total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=l2, solver=saga;, score=0.597 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10, max_iter=100, penalty=l2, solver=saga;, score=0.657 total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=l2, solver=saga;, score=0.472 total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=none, solver=newton-cg;, score=0.584 total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10, max_iter=100, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n[CV 1/3] END C=10, max_iter=100, penalty=none, solver=lbfgs;, score=0.599 total time=   0.1s\n[CV 2/3] END C=10, max_iter=100, penalty=none, solver=lbfgs;, score=0.625 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10, max_iter=100, penalty=none, solver=lbfgs;, score=0.638 total time=   0.1s\n[CV 1/3] END C=10, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=none, solver=sag;, score=0.606 total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=none, solver=sag;, score=0.667 total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=none, solver=sag;, score=0.535 total time=   0.0s\n[CV 1/3] END C=10, max_iter=100, penalty=none, solver=saga;, score=0.597 total time=   0.0s\n[CV 2/3] END C=10, max_iter=100, penalty=none, solver=saga;, score=0.647 total time=   0.0s\n[CV 3/3] END C=10, max_iter=100, penalty=none, solver=saga;, score=0.472 total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10, max_iter=200, penalty=l1, solver=liblinear;, score=0.593 total time=   0.2s\n[CV 2/3] END C=10, max_iter=200, penalty=l1, solver=liblinear;, score=0.644 total time=   0.2s\n[CV 3/3] END C=10, max_iter=200, penalty=l1, solver=liblinear;, score=0.607 total time=   0.1s\n[CV 1/3] END C=10, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=l1, solver=saga;, score=0.606 total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=l1, solver=saga;, score=0.648 total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=l1, solver=saga;, score=0.524 total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=l2, solver=newton-cg;, score=0.619 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=10, max_iter=200, penalty=l2, solver=newton-cg;, score=0.635 total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=l2, solver=newton-cg;, score=0.578 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10, max_iter=200, penalty=l2, solver=lbfgs;, score=0.619 total time=   0.2s\n[CV 2/3] END C=10, max_iter=200, penalty=l2, solver=lbfgs;, score=0.635 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10, max_iter=200, penalty=l2, solver=lbfgs;, score=0.578 total time=   0.2s\n[CV 1/3] END C=10, max_iter=200, penalty=l2, solver=liblinear;, score=0.588 total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=l2, solver=liblinear;, score=0.635 total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=l2, solver=liblinear;, score=0.591 total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=l2, solver=sag;, score=0.600 total time=   0.1s\n[CV 2/3] END C=10, max_iter=200, penalty=l2, solver=sag;, score=0.657 total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=l2, solver=sag;, score=0.574 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10, max_iter=200, penalty=l2, solver=saga;, score=0.606 total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=l2, solver=saga;, score=0.667 total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=l2, solver=saga;, score=0.535 total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=none, solver=newton-cg;, score=0.584 total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=10, max_iter=200, penalty=none, solver=newton-cg;, score=0.642 total time=   0.2s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10, max_iter=200, penalty=none, solver=lbfgs;, score=0.584 total time=   0.2s\n[CV 2/3] END C=10, max_iter=200, penalty=none, solver=lbfgs;, score=0.625 total time=   0.2s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=10, max_iter=200, penalty=none, solver=lbfgs;, score=0.632 total time=   0.2s\n[CV 1/3] END C=10, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=10, max_iter=200, penalty=none, solver=sag;, score=0.600 total time=   0.1s\n[CV 2/3] END C=10, max_iter=200, penalty=none, solver=sag;, score=0.653 total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=none, solver=sag;, score=0.574 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=10, max_iter=200, penalty=none, solver=saga;, score=0.606 total time=   0.0s\n[CV 2/3] END C=10, max_iter=200, penalty=none, solver=saga;, score=0.667 total time=   0.0s\n[CV 3/3] END C=10, max_iter=200, penalty=none, solver=saga;, score=0.535 total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=l1, solver=liblinear;, score=0.584 total time=   0.2s\n[CV 2/3] END C=100, max_iter=50, penalty=l1, solver=liblinear;, score=0.634 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=100, max_iter=50, penalty=l1, solver=liblinear;, score=0.622 total time=   0.2s\n[CV 1/3] END C=100, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=l1, solver=saga;, score=0.611 total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=l1, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=l1, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=l2, solver=newton-cg;, score=0.593 total time=   0.1s\n[CV 2/3] END C=100, max_iter=50, penalty=l2, solver=newton-cg;, score=0.648 total time=   0.1s\n[CV 3/3] END C=100, max_iter=50, penalty=l2, solver=newton-cg;, score=0.607 total time=   0.1s\n[CV 1/3] END C=100, max_iter=50, penalty=l2, solver=lbfgs;, score=0.582 total time=   0.1s\n[CV 2/3] END C=100, max_iter=50, penalty=l2, solver=lbfgs;, score=0.616 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=100, max_iter=50, penalty=l2, solver=lbfgs;, score=0.586 total time=   0.1s\n[CV 1/3] END C=100, max_iter=50, penalty=l2, solver=liblinear;, score=0.584 total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=l2, solver=liblinear;, score=0.644 total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=l2, solver=liblinear;, score=0.618 total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=l2, solver=sag;, score=0.597 total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=l2, solver=sag;, score=0.632 total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=l2, solver=sag;, score=0.472 total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=l2, solver=saga;, score=0.601 total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=l2, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=l2, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=50, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=100, max_iter=50, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=100, max_iter=50, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=50, penalty=none, solver=lbfgs;, score=0.588 total time=   0.1s\n[CV 2/3] END C=100, max_iter=50, penalty=none, solver=lbfgs;, score=0.634 total time=   0.1s\n[CV 3/3] END C=100, max_iter=50, penalty=none, solver=lbfgs;, score=0.578 total time=   0.1s\n[CV 1/3] END C=100, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=none, solver=sag;, score=0.597 total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=none, solver=sag;, score=0.652 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=100, max_iter=50, penalty=none, solver=sag;, score=0.472 total time=   0.0s\n[CV 1/3] END C=100, max_iter=50, penalty=none, solver=saga;, score=0.611 total time=   0.0s\n[CV 2/3] END C=100, max_iter=50, penalty=none, solver=saga;, score=0.560 total time=   0.0s\n[CV 3/3] END C=100, max_iter=50, penalty=none, solver=saga;, score=0.459 total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=100, penalty=l1, solver=liblinear;, score=0.584 total time=   0.2s\n[CV 2/3] END C=100, max_iter=100, penalty=l1, solver=liblinear;, score=0.634 total time=   0.2s\n[CV 3/3] END C=100, max_iter=100, penalty=l1, solver=liblinear;, score=0.622 total time=   0.2s\n[CV 1/3] END C=100, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=l1, solver=saga;, score=0.597 total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=l1, solver=saga;, score=0.647 total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=l1, solver=saga;, score=0.472 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=100, penalty=l2, solver=newton-cg;, score=0.593 total time=   0.2s\n[CV 2/3] END C=100, max_iter=100, penalty=l2, solver=newton-cg;, score=0.648 total time=   0.1s\n[CV 3/3] END C=100, max_iter=100, penalty=l2, solver=newton-cg;, score=0.607 total time=   0.1s\n[CV 1/3] END C=100, max_iter=100, penalty=l2, solver=lbfgs;, score=0.599 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=100, max_iter=100, penalty=l2, solver=lbfgs;, score=0.629 total time=   0.1s\n[CV 3/3] END C=100, max_iter=100, penalty=l2, solver=lbfgs;, score=0.607 total time=   0.1s\n[CV 1/3] END C=100, max_iter=100, penalty=l2, solver=liblinear;, score=0.584 total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=l2, solver=liblinear;, score=0.644 total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=l2, solver=liblinear;, score=0.618 total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=l2, solver=sag;, score=0.606 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=100, max_iter=100, penalty=l2, solver=sag;, score=0.667 total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=l2, solver=sag;, score=0.535 total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=l2, solver=saga;, score=0.597 total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=l2, solver=saga;, score=0.643 total time=   0.1s\n[CV 3/3] END C=100, max_iter=100, penalty=l2, solver=saga;, score=0.472 total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=100, penalty=none, solver=newton-cg;, score=0.584 total time=   0.1s\n[CV 2/3] END C=100, max_iter=100, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=100, max_iter=100, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=100, penalty=none, solver=lbfgs;, score=0.599 total time=   0.1s\n[CV 2/3] END C=100, max_iter=100, penalty=none, solver=lbfgs;, score=0.625 total time=   0.1s\n[CV 3/3] END C=100, max_iter=100, penalty=none, solver=lbfgs;, score=0.638 total time=   0.1s\n[CV 1/3] END C=100, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=none, solver=sag;, score=0.606 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=100, max_iter=100, penalty=none, solver=sag;, score=0.667 total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=none, solver=sag;, score=0.551 total time=   0.0s\n[CV 1/3] END C=100, max_iter=100, penalty=none, solver=saga;, score=0.597 total time=   0.0s\n[CV 2/3] END C=100, max_iter=100, penalty=none, solver=saga;, score=0.638 total time=   0.0s\n[CV 3/3] END C=100, max_iter=100, penalty=none, solver=saga;, score=0.472 total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=l1, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=l1, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=l1, solver=liblinear;, score=0.584 total time=   0.1s\n[CV 2/3] END C=100, max_iter=200, penalty=l1, solver=liblinear;, score=0.634 total time=   0.1s\n[CV 3/3] END C=100, max_iter=200, penalty=l1, solver=liblinear;, score=0.622 total time=   0.2s\n[CV 1/3] END C=100, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=l1, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=l1, solver=saga;, score=0.606 total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=l1, solver=saga;, score=0.667 total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=l1, solver=saga;, score=0.535 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=200, penalty=l2, solver=newton-cg;, score=0.593 total time=   0.1s\n[CV 2/3] END C=100, max_iter=200, penalty=l2, solver=newton-cg;, score=0.648 total time=   0.1s\n[CV 3/3] END C=100, max_iter=200, penalty=l2, solver=newton-cg;, score=0.607 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=200, penalty=l2, solver=lbfgs;, score=0.588 total time=   0.1s\n[CV 2/3] END C=100, max_iter=200, penalty=l2, solver=lbfgs;, score=0.639 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=100, max_iter=200, penalty=l2, solver=lbfgs;, score=0.607 total time=   0.2s\n[CV 1/3] END C=100, max_iter=200, penalty=l2, solver=liblinear;, score=0.584 total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=l2, solver=liblinear;, score=0.644 total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=l2, solver=liblinear;, score=0.618 total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=l2, solver=sag;, score=0.600 total time=   0.1s\n[CV 2/3] END C=100, max_iter=200, penalty=l2, solver=sag;, score=0.657 total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=l2, solver=sag;, score=0.574 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=200, penalty=l2, solver=saga;, score=0.606 total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=l2, solver=saga;, score=0.667 total time=   0.1s\n[CV 3/3] END C=100, max_iter=200, penalty=l2, solver=saga;, score=0.535 total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=elasticnet, solver=newton-cg;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=elasticnet, solver=lbfgs;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=elasticnet, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=elasticnet, solver=sag;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=elasticnet, solver=saga;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=none, solver=newton-cg;, score=0.584 total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=none, solver=newton-cg;, score=0.634 total time=   0.1s\n[CV 3/3] END C=100, max_iter=200, penalty=none, solver=newton-cg;, score=0.642 total time=   0.1s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END C=100, max_iter=200, penalty=none, solver=lbfgs;, score=0.584 total time=   0.1s\n[CV 2/3] END C=100, max_iter=200, penalty=none, solver=lbfgs;, score=0.625 total time=   0.2s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END C=100, max_iter=200, penalty=none, solver=lbfgs;, score=0.632 total time=   0.2s\n[CV 1/3] END C=100, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 2/3] END C=100, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=none, solver=liblinear;, score=nan total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=none, solver=sag;, score=0.600 total time=   0.1s\n[CV 2/3] END C=100, max_iter=200, penalty=none, solver=sag;, score=0.653 total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=none, solver=sag;, score=0.562 total time=   0.0s\n[CV 1/3] END C=100, max_iter=200, penalty=none, solver=saga;, score=0.606 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n486 fits failed out of a total of 1080.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n    raise ValueError(\nValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n    raise ValueError(\nValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n    raise ValueError(\nValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n    raise ValueError(\nValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n    raise ValueError(\nValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 457, in _check_solver\n    raise ValueError(\nValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n    raise ValueError(\nValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1471, in fit\n    raise ValueError(\nValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_sklearn.py\", line 29, in patch_function\n    original_result = original(self, *args, **kwargs)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\nValueError: penalty='none' is not supported for the liblinear solver\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/databricks/python/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.                nan 0.         0.\n 0.         0.23690476 0.         0.                nan        nan\n        nan        nan        nan 0.62002673 0.60016528        nan\n 0.57193349 0.54013833        nan        nan 0.                nan\n 0.         0.         0.         0.23690476 0.         0.\n        nan        nan        nan        nan        nan 0.62002673\n 0.6204071         nan 0.59870333 0.57208298        nan        nan\n 0.                nan 0.         0.         0.         0.23690476\n 0.         0.                nan        nan        nan        nan\n        nan 0.62002673 0.61376485        nan 0.60509259 0.60399434\n        nan        nan 0.                nan 0.         0.31590674\n 0.31590674 0.37060663 0.31227661 0.31929834        nan        nan\n        nan        nan        nan 0.62002673 0.60016528        nan\n 0.56728682 0.54013833        nan        nan 0.                nan\n 0.         0.31590674 0.31590674 0.37060663 0.31467235 0.31227661\n        nan        nan        nan        nan        nan 0.62002673\n 0.6204071         nan 0.60257785 0.56881603        nan        nan\n 0.                nan 0.         0.31590674 0.31590674 0.37060663\n 0.31467235 0.31227661        nan        nan        nan        nan\n        nan 0.62002673 0.61376485        nan 0.61032869 0.60639182\n        nan        nan 0.30761711        nan 0.30761711 0.51347221\n 0.51347221 0.52959933 0.52328073 0.50985982        nan        nan\n        nan        nan        nan 0.62002673 0.60016528        nan\n 0.56881603 0.54013833        nan        nan 0.30761711        nan\n 0.30761711 0.51347221 0.51347221 0.52959933 0.51366539 0.52745867\n        nan        nan        nan        nan        nan 0.62002673\n 0.6204071         nan 0.61014667 0.56881603        nan        nan\n 0.30761711        nan 0.30761711 0.51347221 0.51347221 0.52959933\n 0.50853674 0.51366539        nan        nan        nan        nan\n        nan 0.62002673 0.61376485        nan 0.61556145 0.60399434\n        nan        nan 0.60556802        nan 0.52879105 0.59607384\n 0.5990961  0.60355525 0.55723196 0.54037037        nan        nan\n        nan        nan        nan 0.62002673 0.60016528        nan\n 0.56881603 0.54013833        nan        nan 0.60556802        nan\n 0.56120335 0.59607384 0.59607384 0.60355525 0.58426207 0.56205876\n        nan        nan        nan        nan        nan 0.62002673\n 0.6204071         nan 0.60639182 0.56881603        nan        nan\n 0.60556802        nan 0.56426919 0.59607384 0.59607384 0.60355525\n 0.60815728 0.58100753        nan        nan        nan        nan\n        nan 0.62002673 0.61376485        nan 0.60509259 0.60257785\n        nan        nan 0.61461187        nan 0.5358932  0.61053932\n 0.61135024 0.60475984 0.56728682 0.54337583        nan        nan\n        nan        nan        nan 0.62002673 0.60016528        nan\n 0.57054136 0.54337583        nan        nan 0.61461187        nan\n 0.56521087 0.61053932 0.61050642 0.60475984 0.59870333 0.57523375\n        nan        nan        nan        nan        nan 0.62002673\n 0.6204071         nan 0.60257785 0.57208298        nan        nan\n 0.61461187        nan 0.59244355 0.61053932 0.61053932 0.60475984\n 0.61032869 0.60257785        nan        nan        nan        nan\n        nan 0.62002673 0.61376485        nan 0.60880706 0.60257785\n        nan        nan 0.61332221        nan 0.54337583 0.61609195\n 0.59499802 0.61514143 0.56703996 0.54013833        nan        nan\n        nan        nan        nan 0.62002673 0.60016528        nan\n 0.57364695 0.54337583        nan        nan 0.61332221        nan\n 0.57208298 0.61609195 0.61177273 0.61514143 0.60257785 0.57054136\n        nan        nan        nan        nan        nan 0.62002673\n 0.6204071         nan 0.60782719 0.56881603        nan        nan\n 0.61332221        nan 0.60257785 0.61609195 0.61151053 0.61514143\n 0.61032869 0.60257785        nan        nan        nan        nan\n        nan 0.62002673 0.61376485        nan 0.60509259 0.60639182]\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END C=100, max_iter=200, penalty=none, solver=saga;, score=0.667 total time=   0.0s\n[CV 3/3] END C=100, max_iter=200, penalty=none, solver=saga;, score=0.547 total time=   0.0s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n[CV 1/3] END n_neighbors=3, p=1, weights=uniform;, score=0.571 total time=   0.0s\n[CV 2/3] END n_neighbors=3, p=1, weights=uniform;, score=0.585 total time=   0.0s\n[CV 3/3] END n_neighbors=3, p=1, weights=uniform;, score=0.618 total time=   0.0s\n[CV 1/3] END n_neighbors=3, p=1, weights=distance;, score=0.571 total time=   0.0s\n[CV 2/3] END n_neighbors=3, p=1, weights=distance;, score=0.577 total time=   0.0s\n[CV 3/3] END n_neighbors=3, p=1, weights=distance;, score=0.614 total time=   0.0s\n[CV 1/3] END n_neighbors=3, p=2, weights=uniform;, score=0.558 total time=   0.0s\n[CV 2/3] END n_neighbors=3, p=2, weights=uniform;, score=0.583 total time=   0.0s\n[CV 3/3] END n_neighbors=3, p=2, weights=uniform;, score=0.591 total time=   0.0s\n[CV 1/3] END n_neighbors=3, p=2, weights=distance;, score=0.558 total time=   0.0s\n[CV 2/3] END n_neighbors=3, p=2, weights=distance;, score=0.583 total time=   0.0s\n[CV 3/3] END n_neighbors=3, p=2, weights=distance;, score=0.591 total time=   0.0s\n[CV 1/3] END n_neighbors=5, p=1, weights=uniform;, score=0.590 total time=   0.0s\n[CV 2/3] END n_neighbors=5, p=1, weights=uniform;, score=0.542 total time=   0.0s\n[CV 3/3] END n_neighbors=5, p=1, weights=uniform;, score=0.623 total time=   0.0s\n[CV 1/3] END n_neighbors=5, p=1, weights=distance;, score=0.599 total time=   0.0s\n[CV 2/3] END n_neighbors=5, p=1, weights=distance;, score=0.548 total time=   0.0s\n[CV 3/3] END n_neighbors=5, p=1, weights=distance;, score=0.614 total time=   0.0s\n[CV 1/3] END n_neighbors=5, p=2, weights=uniform;, score=0.561 total time=   0.0s\n[CV 2/3] END n_neighbors=5, p=2, weights=uniform;, score=0.601 total time=   0.0s\n[CV 3/3] END n_neighbors=5, p=2, weights=uniform;, score=0.587 total time=   0.0s\n[CV 1/3] END n_neighbors=5, p=2, weights=distance;, score=0.561 total time=   0.0s\n[CV 2/3] END n_neighbors=5, p=2, weights=distance;, score=0.601 total time=   0.0s\n[CV 3/3] END n_neighbors=5, p=2, weights=distance;, score=0.591 total time=   0.0s\n[CV 1/3] END n_neighbors=7, p=1, weights=uniform;, score=0.595 total time=   0.0s\n[CV 2/3] END n_neighbors=7, p=1, weights=uniform;, score=0.549 total time=   0.0s\n[CV 3/3] END n_neighbors=7, p=1, weights=uniform;, score=0.624 total time=   0.0s\n[CV 1/3] END n_neighbors=7, p=1, weights=distance;, score=0.595 total time=   0.0s\n[CV 2/3] END n_neighbors=7, p=1, weights=distance;, score=0.549 total time=   0.0s\n[CV 3/3] END n_neighbors=7, p=1, weights=distance;, score=0.624 total time=   0.0s\n[CV 1/3] END n_neighbors=7, p=2, weights=uniform;, score=0.564 total time=   0.0s\n[CV 2/3] END n_neighbors=7, p=2, weights=uniform;, score=0.544 total time=   0.0s\n[CV 3/3] END n_neighbors=7, p=2, weights=uniform;, score=0.604 total time=   0.1s\n[CV 1/3] END n_neighbors=7, p=2, weights=distance;, score=0.577 total time=   0.1s\n[CV 2/3] END n_neighbors=7, p=2, weights=distance;, score=0.554 total time=   0.0s\n[CV 3/3] END n_neighbors=7, p=2, weights=distance;, score=0.608 total time=   0.0s\nFitting 3 folds for each of 10 candidates, totalling 30 fits\n[CV 1/3] END ...............var_smoothing=1e-09;, score=0.647 total time=   0.0s\n[CV 2/3] END ...............var_smoothing=1e-09;, score=0.697 total time=   0.0s\n[CV 3/3] END ...............var_smoothing=1e-09;, score=0.654 total time=   0.0s\n[CV 1/3] END ...............var_smoothing=1e-08;, score=0.647 total time=   0.0s\n[CV 2/3] END ...............var_smoothing=1e-08;, score=0.697 total time=   0.0s\n[CV 3/3] END ...............var_smoothing=1e-08;, score=0.653 total time=   0.0s\n[CV 1/3] END ...............var_smoothing=1e-07;, score=0.647 total time=   0.0s\n[CV 2/3] END ...............var_smoothing=1e-07;, score=0.697 total time=   0.0s\n[CV 3/3] END ...............var_smoothing=1e-07;, score=0.660 total time=   0.0s\n[CV 1/3] END ...............var_smoothing=1e-06;, score=0.647 total time=   0.0s\n[CV 2/3] END ...............var_smoothing=1e-06;, score=0.697 total time=   0.0s\n[CV 3/3] END ...............var_smoothing=1e-06;, score=0.663 total time=   0.0s\n[CV 1/3] END ...............var_smoothing=1e-05;, score=0.647 total time=   0.0s\n[CV 2/3] END ...............var_smoothing=1e-05;, score=0.685 total time=   0.0s\n[CV 3/3] END ...............var_smoothing=1e-05;, score=0.696 total time=   0.0s\n[CV 1/3] END ..............var_smoothing=0.0001;, score=0.655 total time=   0.0s\n[CV 2/3] END ..............var_smoothing=0.0001;, score=0.681 total time=   0.0s\n[CV 3/3] END ..............var_smoothing=0.0001;, score=0.697 total time=   0.0s\n[CV 1/3] END ...............var_smoothing=0.001;, score=0.656 total time=   0.0s\n[CV 2/3] END ...............var_smoothing=0.001;, score=0.660 total time=   0.0s\n[CV 3/3] END ...............var_smoothing=0.001;, score=0.679 total time=   0.0s\n[CV 1/3] END ................var_smoothing=0.01;, score=0.660 total time=   0.0s\n[CV 2/3] END ................var_smoothing=0.01;, score=0.697 total time=   0.0s\n[CV 3/3] END ................var_smoothing=0.01;, score=0.643 total time=   0.0s\n[CV 1/3] END .................var_smoothing=0.1;, score=0.602 total time=   0.0s\n[CV 2/3] END .................var_smoothing=0.1;, score=0.000 total time=   0.0s\n[CV 3/3] END .................var_smoothing=0.1;, score=0.533 total time=   0.0s\n[CV 1/3] END ...................var_smoothing=1;, score=0.132 total time=   0.0s\n[CV 2/3] END ...................var_smoothing=1;, score=0.000 total time=   0.0s\n[CV 3/3] END ...................var_smoothing=1;, score=0.423 total time=   0.0s\nFitting 3 folds for each of 72 candidates, totalling 216 fits\n[CV 1/3] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.521 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.588 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.500 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5;, score=0.504 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5;, score=0.563 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5;, score=0.413 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.485 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.610 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.438 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=2;, score=0.556 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=2;, score=0.508 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=2;, score=0.463 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5;, score=0.562 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5;, score=0.528 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5;, score=0.416 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10;, score=0.523 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10;, score=0.588 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10;, score=0.423 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.593 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.638 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.504 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=5;, score=0.599 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=5;, score=0.613 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=5;, score=0.525 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.594 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.619 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.489 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2;, score=0.548 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2;, score=0.662 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2;, score=0.437 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=5;, score=0.565 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=5;, score=0.646 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=5;, score=0.437 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10;, score=0.561 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10;, score=0.667 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10;, score=0.430 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=2;, score=0.565 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=2;, score=0.662 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=2;, score=0.437 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=5;, score=0.565 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=5;, score=0.654 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=5;, score=0.437 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=10;, score=0.565 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=10;, score=0.671 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=10;, score=0.433 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=2;, score=0.547 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=2;, score=0.679 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=2;, score=0.433 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=5;, score=0.578 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=5;, score=0.679 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=5;, score=0.433 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=10;, score=0.578 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=10;, score=0.671 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=10;, score=0.433 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2;, score=0.543 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2;, score=0.582 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2;, score=0.420 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=5;, score=0.612 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=5;, score=0.571 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=5;, score=0.410 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10;, score=0.522 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10;, score=0.607 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10;, score=0.453 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2;, score=0.584 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2;, score=0.532 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2;, score=0.439 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5;, score=0.541 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5;, score=0.496 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5;, score=0.439 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10;, score=0.541 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10;, score=0.586 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10;, score=0.435 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=2;, score=0.604 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=2;, score=0.619 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=2;, score=0.511 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=5;, score=0.600 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=5;, score=0.603 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=5;, score=0.504 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=10;, score=0.600 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=10;, score=0.620 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=10;, score=0.485 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=2;, score=0.569 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=2;, score=0.543 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=2;, score=0.472 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=5;, score=0.522 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=5;, score=0.559 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=5;, score=0.441 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=10;, score=0.489 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=10;, score=0.610 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=10;, score=0.429 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=15, min_samples_leaf=2, min_samples_split=2;, score=0.574 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=15, min_samples_leaf=2, min_samples_split=2;, score=0.512 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=15, min_samples_leaf=2, min_samples_split=2;, score=0.390 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=15, min_samples_leaf=2, min_samples_split=5;, score=0.534 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=15, min_samples_leaf=2, min_samples_split=5;, score=0.528 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=15, min_samples_leaf=2, min_samples_split=5;, score=0.387 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=15, min_samples_leaf=2, min_samples_split=10;, score=0.523 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=15, min_samples_leaf=2, min_samples_split=10;, score=0.588 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=15, min_samples_leaf=2, min_samples_split=10;, score=0.457 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=15, min_samples_leaf=4, min_samples_split=2;, score=0.593 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=15, min_samples_leaf=4, min_samples_split=2;, score=0.613 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=15, min_samples_leaf=4, min_samples_split=2;, score=0.511 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=15, min_samples_leaf=4, min_samples_split=5;, score=0.603 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=15, min_samples_leaf=4, min_samples_split=5;, score=0.600 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=15, min_samples_leaf=4, min_samples_split=5;, score=0.514 total time=   0.0s\n[CV 1/3] END criterion=gini, max_depth=15, min_samples_leaf=4, min_samples_split=10;, score=0.588 total time=   0.0s\n[CV 2/3] END criterion=gini, max_depth=15, min_samples_leaf=4, min_samples_split=10;, score=0.624 total time=   0.0s\n[CV 3/3] END criterion=gini, max_depth=15, min_samples_leaf=4, min_samples_split=10;, score=0.485 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.526 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.580 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2;, score=0.427 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=5;, score=0.515 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=5;, score=0.565 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=5;, score=0.466 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.533 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.628 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10;, score=0.427 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=2;, score=0.551 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=2;, score=0.532 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=2;, score=0.485 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5;, score=0.511 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5;, score=0.528 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5;, score=0.493 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10;, score=0.559 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10;, score=0.642 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10;, score=0.463 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.534 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.609 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=2;, score=0.481 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=5;, score=0.541 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=5;, score=0.588 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=5;, score=0.507 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.574 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.613 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=10;, score=0.496 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2;, score=0.520 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2;, score=0.649 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2;, score=0.442 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=5;, score=0.528 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=5;, score=0.649 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=5;, score=0.439 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10;, score=0.528 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10;, score=0.662 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10;, score=0.435 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=2;, score=0.532 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=2;, score=0.654 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=2;, score=0.439 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=5;, score=0.532 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=5;, score=0.654 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=5;, score=0.435 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=10;, score=0.532 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=10;, score=0.662 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=10;, score=0.435 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=2;, score=0.545 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=2;, score=0.662 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=2;, score=0.435 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=5;, score=0.545 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=5;, score=0.662 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=5;, score=0.435 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=10;, score=0.512 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=10;, score=0.662 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=10;, score=0.435 total time=   0.0s\n[CV 1/3] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2;, score=0.531 total time=   0.0s\n[CV 2/3] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2;, score=0.590 total time=   0.0s\n[CV 3/3] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2;, score=0.514 total time=   0.\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nators=100, subsample=0.8;, score=0.609 total time=   0.1s\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=100, subsample=1.0;, score=0.609 total time=   0.1s\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=100, subsample=1.0;, score=0.704 total time=   0.1s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=100, subsample=1.0;, score=0.593 total time=   0.1s\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.8;, score=0.569 total time=   0.1s\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.8;, score=0.667 total time=   0.2s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.8;, score=0.593 total time=   0.1s\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=1.0;, score=0.548 total time=   0.2s\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=1.0;, score=0.676 total time=   0.1s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=1.0;, score=0.582 total time=   0.1s\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.8;, score=0.580 total time=   0.1s\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.8;, score=0.643 total time=   0.1s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.8;, score=0.606 total time=   0.1s\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=1.0;, score=0.590 total time=   0.1s\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=1.0;, score=0.643 total time=   0.1s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=1.0;, score=0.588 total time=   0.1s\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8;, score=0.576 total time=   0.1s\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8;, score=0.628 total time=   0.1s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8;, score=0.602 total time=   0.1s\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0;, score=0.552 total time=   0.1s\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0;, score=0.638 total time=   0.1s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0;, score=0.567 total time=   0.1s\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8;, score=0.597 total time=   0.2s\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8;, score=0.647 total time=   0.2s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8;, score=0.595 total time=   0.2s\n[CV 1/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0;, score=0.580 total time=   0.2s\n[CV 2/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0;, score=0.627 total time=   0.2s\n[CV 3/3] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0;, score=0.545 total time=   0.3s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.8;, score=0.375 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.8;, score=0.273 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.8;, score=0.198 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0;, score=0.504 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0;, score=0.227 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0;, score=0.419 total time=   0.0s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8;, score=0.571 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8;, score=0.574 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8;, score=0.429 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0;, score=0.555 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0;, score=0.571 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0;, score=0.452 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8;, score=0.647 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8;, score=0.662 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8;, score=0.576 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0;, score=0.574 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0;, score=0.639 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0;, score=0.535 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=0.8;, score=0.486 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=0.8;, score=0.417 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=0.8;, score=0.298 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=1.0;, score=0.486 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=1.0;, score=0.436 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=1.0;, score=0.371 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8;, score=0.621 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8;, score=0.600 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8;, score=0.521 total time=   0.3s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1.0;, score=0.587 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1.0;, score=0.571 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1.0;, score=0.462 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=0.8;, score=0.638 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=0.8;, score=0.681 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=0.8;, score=0.582 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=1.0;, score=0.588 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=1.0;, score=0.611 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=1.0;, score=0.540 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.8;, score=0.527 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.8;, score=0.514 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.8;, score=0.380 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=1.0;, score=0.509 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=1.0;, score=0.495 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=1.0;, score=0.455 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8;, score=0.621 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8;, score=0.636 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8;, score=0.520 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0;, score=0.595 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0;, score=0.647 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0;, score=0.523 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8;, score=0.628 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8;, score=0.638 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8;, score=0.584 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0;, score=0.556 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0;, score=0.648 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0;, score=0.559 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.632 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.681 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.8;, score=0.574 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1.0;, score=0.582 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1.0;, score=0.657 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1.0;, score=0.614 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.624 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.667 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8;, score=0.580 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0;, score=0.624 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0;, score=0.634 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0;, score=0.593 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8;, score=0.614 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8;, score=0.606 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8;, score=0.627 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0;, score=0.619 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0;, score=0.652 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0;, score=0.571 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.604 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.643 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=0.8;, score=0.591 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=1.0;, score=0.597 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=1.0;, score=0.648 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=1.0;, score=0.593 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.597 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.620 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8;, score=0.612 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1.0;, score=0.613 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1.0;, score=0.676 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1.0;, score=0.586 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8;, score=0.582 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8;, score=0.604 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8;, score=0.582 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=1.0;, score=0.599 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=1.0;, score=0.676 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=1.0;, score=0.582 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.639 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.643 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.8;, score=0.618 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=1.0;, score=0.624 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=1.0;, score=0.690 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=1.0;, score=0.590 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.596 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.648 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8;, score=0.595 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0;, score=0.578 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0;, score=0.671 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0;, score=0.569 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8;, score=0.586 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8;, score=0.624 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8;, score=0.571 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0;, score=0.609 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0;, score=0.662 total time=   0.2s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0;, score=0.574 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=50, subsample=0.8;, score=0.606 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=50, subsample=0.8;, score=0.624 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=50, subsample=0.8;, score=0.613 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=50, subsample=1.0;, score=0.623 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=50, subsample=1.0;, score=0.676 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=50, subsample=1.0;, score=0.580 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8;, score=0.580 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8;, score=0.615 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8;, score=0.574 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0;, score=0.616 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0;, score=0.629 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0;, score=0.578 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8;, score=0.611 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8;, score=0.590 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8;, score=0.578 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0;, score=0.591 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0;, score=0.604 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0;, score=0.569 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=50, subsample=0.8;, score=0.584 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=50, subsample=0.8;, score=0.611 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=50, subsample=0.8;, score=0.567 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=50, subsample=1.0;, score=0.594 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=50, subsample=1.0;, score=0.704 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=50, subsample=1.0;, score=0.619 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.8;, score=0.569 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.8;, score=0.610 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=100, subsample=0.8;, score=0.571 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=100, subsample=1.0;, score=0.586 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=100, subsample=1.0;, score=0.695 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=100, subsample=1.0;, score=0.600 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.8;, score=0.580 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.8;, score=0.609 total time=   0.3s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=0.8;, score=0.561 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=1.0;, score=0.586 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=1.0;, score=0.667 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=4, n_estimators=200, subsample=1.0;, score=0.584 total time=   0.2s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.8;, score=0.606 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.8;, score=0.633 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.8;, score=0.597 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=1.0;, score=0.604 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=1.0;, score=0.690 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=1.0;, score=0.612 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8;, score=0.599 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8;, score=0.609 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8;, score=0.588 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0;, score=0.638 total time=   0.1s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0;, score=0.676 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0;, score=0.597 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8;, score=0.590 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8;, score=0.603 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8;, score=0.571 total time=   0.1s\n[CV 1/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0;, score=0.613 total time=   0.2s\n[CV 2/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0;, score=0.647 total time=   0.1s\n[CV 3/3] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0;, score=0.612 total time=   0.2s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import mlflow as mlflow\n",
    "l=[]\n",
    "for i in ['LogisticRegression','KNeighborsClassifier','GaussianNB','DecisionTreeClassifier','RandomForestClassifier','XGBClassifier']:\n",
    "    with mlflow.start_run(experiment_id=\"1217255725030073\"):\n",
    "        params={'RandomForestClassifier':\n",
    "                    {'n_estimators':[400,600,700,1000,1200],'max_depth':[4,5,6,7,8],'criterion':['entropy']},\n",
    "                'KNeighborsClassifier':\n",
    "                    {'n_neighbors': [3, 5, 7],'weights': ['uniform', 'distance'],'p': [1, 2]},\n",
    "                'DecisionTreeClassifier':\n",
    "                    {'criterion': ['gini', 'entropy'],'max_depth': [None, 5, 10, 15],'min_samples_split': [2, 5, 10],'min_samples_leaf': [1, 2, 4]},\n",
    "                'LogisticRegression':\n",
    "                    {'penalty': ['l1', 'l2', 'elasticnet', 'none'],'C': [0.001, 0.01, 0.1, 1, 10, 100],'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],'max_iter': [50, 100, 200]},\n",
    "                'GaussianNB': \n",
    "                    {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]},\n",
    "                'XGBClassifier':\n",
    "                    {'learning_rate': [0.01, 0.1, 0.2],'n_estimators': [50, 100, 200],'max_depth': [3, 4, 5],'subsample': [0.8, 1.0],'colsample_bytree': [0.8, 1.0]}  \n",
    "                }\n",
    "                                    \n",
    "        gscv=GridSearchCV(estimator=globals()[i](),param_grid=params[i],scoring='f1',cv=3,verbose=3)\n",
    "        gscv.fit(xtrain,ytrain)\n",
    "        model=globals()[i](**gscv.best_params_)\n",
    "        model.fit(xtrain,ytrain)\n",
    "        y_pred=model.predict(xtest)\n",
    "        l.append({\n",
    "            'model':i,\n",
    "            'recall':recall_score(ytest,y_pred),\n",
    "            'precision':precision_score(ytest,y_pred),\n",
    "            'f1_score':f1_score(ytest,y_pred)\n",
    "        })\n",
    "        mlflow.log_params({\n",
    "            'model':i,\n",
    "            'recall':recall_score(ytest,y_pred),\n",
    "            'precision':precision_score(ytest,y_pred),\n",
    "            'f1_score':f1_score(ytest,y_pred)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deff4fb3-14c8-4cdd-adce-ae8881b783f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.695187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.632911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.649351</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.606061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>0.624204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.610390</td>\n",
       "      <td>0.635135</td>\n",
       "      <td>0.622517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.549296</td>\n",
       "      <td>0.527027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>f1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>GaussianNB</td>\n      <td>0.844156</td>\n      <td>0.590909</td>\n      <td>0.695187</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>LogisticRegression</td>\n      <td>0.649351</td>\n      <td>0.617284</td>\n      <td>0.632911</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>KNeighborsClassifier</td>\n      <td>0.649351</td>\n      <td>0.568182</td>\n      <td>0.606061</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>XGBClassifier</td>\n      <td>0.636364</td>\n      <td>0.612500</td>\n      <td>0.624204</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RandomForestClassifier</td>\n      <td>0.610390</td>\n      <td>0.635135</td>\n      <td>0.622517</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DecisionTreeClassifier</td>\n      <td>0.506494</td>\n      <td>0.549296</td>\n      <td>0.527027</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(l).sort_values('recall',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8431fc31-f4b8-41f1-8162-ae2087b4cc43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDUCATIONAL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
